<br>
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!unzip dogs.zip

img_heights, img_width = 180, 180
batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
    "dogs/train",
    image_size = (img_heights, img_width),
    batch_size = batch_size
)
test_ds = tf.keras.utils.image_dataset_from_directory(
    "dogs/test",
    image_size = (img_heights, img_width),
    batch_size = batch_size
)
val_ds = tf.keras.utils.image_dataset_from_directory(
    "dogs/valid",
    image_size = (img_heights, img_width),
    batch_size = batch_size
)

class_name = ["Afghan", "African Wild Dog", "Airedale", "American Hairless", "American Spaniel", 
"Basenji", "Basset", "Beagle", "Bearded Collie", "Bermaise", "Bichon Frise,", "Blenheim", 
"Bloodhound", "Bluetick", "Border Collie", "Borzoi", "Boston Terrier", "Boxer", 
"Bull Mastiff", "Bull Terrier", "Bulldog", "Cairn", "Chihuahua", "Chinese Crested", "Chow", 
"Clumber", "Cockapoo", "Cocker", "Collie", "Corgi", "Coyote", "Dalmation", "Dhole", "Dingo", 
"Doberman", "Elk Hound", "French Bulldog", "German Sheperd", "Golden Retriever", 
"Great Dane", "Great Perenees", "Greyhound", "Groenendael", "Irish Spaniel", 
"Irish Wolfhound", "Japanese Spaniel", "Komondor", "Labradoodle", "Labrador", "Lhasa", 
"Malinois", "Maltese", "Mex Hairless", "Newfoundland", "Pekinese", "Pit Bull", "Pomeranian", 
"Poodle", "Pug", "Rhodesian", "Rottweiler", "Saint Bernard", "Schnauzer", "Scotch Terrier", 
"Shar_Pei", "Shiba Inu", "Shih-Tzu", "Siberian Husky", "Vizsla", "Yorkie"]
plt.figure(figsize=(10,10))
for images, labels in train_ds.take(1):
  for i in range(16):
    ax = plt.subplot(4,4, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_name[labels[i]])
    plt.axis("off")

data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_heights,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)

model = Sequential([
  data_augmentation,
  layers.Rescaling(1./255),
  layers.Conv2D(32, kernel_size=(3, 3), input_shape=(100, 100, 3), activation='relu'),
  layers.MaxPooling2D(pool_size=2),
  layers.Dropout(0.2),

  layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
  layers.MaxPooling2D(pool_size=2),
  layers.Dropout(0.2),

  layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),
  layers.MaxPooling2D(pool_size=2),
  layers.Dropout(0.2),

  layers.Flatten(),
  layers.Dense(256, activation='relu'),
  layers.Dropout(0.5),
  layers.Dense(70, activation='softmax'),
])

model.compile(
    optimizer="adam",
    loss=tf.losses.SparseCategoricalCrossentropy(from_logits = True),
    metrics=['accuracy']
)

history = model.fit(
    train_ds,
    validation_data = val_ds,
    epochs = 30
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(30)

plt.figure(figsize=(15, 15))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

model.evaluate(val_ds)

plt.figure(figsize=(15,15))
for images, labels in val_ds.take(1):
  classifications = model(images)
  # print(classifications)
  for i in range(9):
    ax = plt.subplot(3,3, i+1)
    plt.imshow(images[i].numpy().astype("uint8"))
    index = numpy.argmax(classifications[i])
    plt.title("Pred: " + class_name[index] + " | Real: " + class_name[labels[i]])

dog_url = "https://www.thesprucepets.com/thmb/9UFLqcnM5C99ww1-N_c5B39u_6k=/4367x2456/
smart/filters:no_upscale()/beagle-RolfKopfle-Photolibrary-Getty-135631212-56a26b1d3df78cf772756667.jpg"
dog_path = tf.keras.utils.get_file('', origin=dog_url)

img = tf.keras.utils.load_img(
    dog_path, target_size=(img_heights, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_name[np.argmax(score)], 100 * np.max(score))
)